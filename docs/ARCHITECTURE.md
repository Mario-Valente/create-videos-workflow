# ARCHITECTURE.md - Arquitetura TÃ©cnica

## ğŸ“ VisÃ£o Geral

Pipeline 100% local para geraÃ§Ã£o automÃ¡tica de vÃ­deos usando IA generativa:

```
Input (Tema)
    â†“
[1. Planejamento] â†’ Ollama
    â†“
[2. Roteiro] â†’ Ollama
    â†“
[3. NarraÃ§Ã£o] â†’ Piper TTS
    â†“
[4. Prompts] â†’ Ollama
    â†“
[5. Imagens] â†’ Stable Diffusion
    â†“
[6. Legendas] â†’ Python (processamento de texto)
    â†“
[7. ComposiÃ§Ã£o] â†’ FFmpeg
    â†“
Output (VÃ­deo MP4)
```

## ğŸ—ï¸ Componentes Principais

### 1. **Orchestrator** (`orchestrator.py`)
- Coordena execuÃ§Ã£o de todos os 7 steps
- Gerencia logging e resultados
- Permite skip de steps individuais
- Rastreia tempo de execuÃ§Ã£o

### 2. **Utils** (`scripts/utils.py`)
Classes compartilhadas:

- **OllamaClient**: Interface com Ollama (POST requests)
- **FileManager**: Gerencia estrutura de arquivos de saÃ­da
- **ConfigManager**: Carrega configuraÃ§Ãµes YAML
- **ScriptParser**: Parseia markdown para extrair cenas
- **TimestampExtractor**: Extrai duraÃ§Ã£o de Ã¡udio
- **VideoComposer**: ConstrÃ³i comandos FFmpeg
- **StableDiffusionGenerator**: Interface com WebUI

### 3. **Scripts de Processamento** (7 etapas)

```
01_plan.py
â”œâ”€â”€ Input: Tema (string)
â”œâ”€â”€ LLM: Ollama Mistral
â”œâ”€â”€ Output: plan.json
â”‚   â”œâ”€â”€ tema
â”‚   â”œâ”€â”€ publico
â”‚   â”œâ”€â”€ tom
â”‚   â”œâ”€â”€ pontos_chave[]
â”‚   â””â”€â”€ hook_inicial
â””â”€â”€ Tempo: 10-30s

02_script.py
â”œâ”€â”€ Input: plan.json
â”œâ”€â”€ LLM: Ollama Mistral
â”œâ”€â”€ Output: script.md
â”‚   â”œâ”€â”€ ## CENA 1 (0-10s)
â”‚   â”œâ”€â”€ **NarraÃ§Ã£o:** ...
â”‚   â””â”€â”€ **Visual:** ...
â””â”€â”€ Tempo: 30-60s

03_voice.py
â”œâ”€â”€ Input: script.md
â”œâ”€â”€ TTS: Piper (pt_BR)
â”œâ”€â”€ Output:
â”‚   â”œâ”€â”€ audio/narration.wav
â”‚   â””â”€â”€ timestamps.json
â””â”€â”€ Tempo: 5-15s

04_image_prompts.py
â”œâ”€â”€ Input: script.md + plan.json
â”œâ”€â”€ LLM: Ollama Mistral
â”œâ”€â”€ Output: image_prompts.json
â”‚   â”œâ”€â”€ cenas[]
â”‚   â”œâ”€â”€ prompt_otimizado
â”‚   â””â”€â”€ duracao
â””â”€â”€ Tempo: 30-60s

05_generate_images.py
â”œâ”€â”€ Input: image_prompts.json
â”œâ”€â”€ Image Gen: Stable Diffusion
â”œâ”€â”€ Output: images/scene_001.png ... scene_N.png
â”‚   â”œâ”€â”€ ResoluÃ§Ã£o: 1920Ã—1080
â”‚   â”œâ”€â”€ Formato: PNG
â”‚   â””â”€â”€ Steps: 15-25
â””â”€â”€ Tempo: 2-10 min por imagem

06_subtitles.py
â”œâ”€â”€ Input: script.md
â”œâ”€â”€ Processamento: Regex + split
â”œâ”€â”€ Output:
â”‚   â”œâ”€â”€ subtitles.srt
â”‚   â””â”€â”€ subtitles.vtt
â””â”€â”€ Tempo: 5-10s

07_compose_video.py
â”œâ”€â”€ Input:
â”‚   â”œâ”€â”€ images/scene_*.png
â”‚   â”œâ”€â”€ audio/narration.wav
â”‚   â””â”€â”€ subtitles.srt
â”œâ”€â”€ ComposiÃ§Ã£o: FFmpeg
â”œâ”€â”€ Output: video_final.mp4
â”‚   â”œâ”€â”€ Codec: H.264
â”‚   â”œâ”€â”€ FPS: 30
â”‚   â””â”€â”€ CRF: 18
â””â”€â”€ Tempo: 1-3 min
```

## ğŸ”„ Fluxo de Dados

### Estrutura de DiretÃ³rios

```
output/{timestamp}_tema/
â”œâ”€â”€ plan.json                    # [01] Input: tema â†’ Output: estrutura
â”œâ”€â”€ script.md                    # [02] Output: roteiro em markdown
â”œâ”€â”€ image_prompts.json           # [04] Output: prompts otimizados
â”œâ”€â”€ timestamps.json              # [03] Output: timing de Ã¡udio
â”œâ”€â”€ subtitles.srt               # [06] Output: legendas SRT
â”œâ”€â”€ subtitles.vtt               # [06] Output: legendas VTT
â”œâ”€â”€ audio/
â”‚   â””â”€â”€ narration.wav           # [03] Output: Ã¡udio narraÃ§Ã£o
â”œâ”€â”€ images/
â”‚   â”œâ”€â”€ scene_001.png           # [05] Output: imagens geradas
â”‚   â”œâ”€â”€ scene_002.png
â”‚   â””â”€â”€ ...
â”œâ”€â”€ video_final.mp4             # [07] Output: vÃ­deo final
â”œâ”€â”€ pipeline.log                # Log detalhado de execuÃ§Ã£o
â””â”€â”€ pipeline_results.json       # Resumo de resultados
```

## ğŸ”— DependÃªncias Externas

### **Ollama (LLM Local)**
```
POST http://localhost:11434/api/generate
{
  "model": "mistral",
  "prompt": "...",
  "stream": false,
  "temperature": 0.7
}
```

Modelos disponÃ­veis:
- **mistral** (7B): RÃ¡pido, versÃ¡til â†’ DEFAULT
- **llama2** (7B): Mais preciso
- **neural-chat** (7B): Conversacional

### **Piper TTS**
```bash
piper --model pt_BR-faber-medium \
      --input-file script.txt \
      --output-file narration.wav
```

Formatos suportados:
- pt_BR (portuguÃªs), en_US, es_ES, fr_FR, etc
- Vozes: faber-medium, faber-large, ljspeech-high

### **Stable Diffusion**
```
POST http://127.0.0.1:7860/api/v1/txt2img
{
  "prompt": "...",
  "steps": 25,
  "width": 1920,
  "height": 1080,
  "cfg_scale": 7.5
}
```

Resposta: Base64-encoded PNG

### **FFmpeg**
```bash
ffmpeg -framerate 30 \
       -i images/scene_%03d.png \
       -i audio/narration.wav \
       -vf subtitles=subtitles.srt \
       -c:v libx264 -crf 18 \
       video_final.mp4
```

## ğŸ“Š Processamento por Step

### Step 1: Planejamento
**Input**: Tema (string)
**Processamento**:
```python
prompt = PLAN_PROMPT.format(topic=topic)
plan = ollama.generate_json(prompt, model="mistral")
# Validar campos obrigatÃ³rios
# Salvar JSON
```

### Step 2: Roteiro
**Input**: plan.json
**Processamento**:
```python
# Ler plan.json
# Construir prompt com contexto
# Gerar markdown com regex pattern: "## CENA (\d+) \((\d+)-(\d+)s\)"
# Extrair narraÃ§Ã£o e visual de cada cena
```

### Step 3: NarraÃ§Ã£o
**Input**: script.md
**Processamento**:
```python
# Regex: extrair **NarraÃ§Ã£o:** lines
# Concatenar em texto Ãºnico
# Executar Piper TTS
# Extrair duraÃ§Ã£o com librosa
# Gerar timestamps.json
```

### Step 4: Prompts
**Input**: script.md + plan.json
**Processamento**:
```python
# Extrair descriÃ§Ãµes visuais (**Visual:** lines)
# Para cada visual:
#   - Construir prompt de otimizaÃ§Ã£o
#   - Chamar Ollama
#   - Salvar prompt otimizado
```

### Step 5: Imagens
**Input**: image_prompts.json
**Processamento**:
```python
# Para cada cena:
#   - Conectar ao Stable Diffusion
#   - POST txt2img com prompt
#   - Decodificar Base64
#   - Salvar PNG com padding (scene_001.png)
```

### Step 6: Legendas
**Input**: script.md
**Processamento**:
```python
# Extrair cenas e timing: ## CENA (\d+) \((\d+)-(\d+)s\)
# Para cada cena:
#   - Dividir narraÃ§Ã£o em linhas (max 42 chars)
#   - Calcular timing por linha
#   - Formato SRT: index, timestamps, text
#   - Formato VTT: timestamps com ".mmm", text
```

### Step 7: ComposiÃ§Ã£o
**Input**: images/, audio/narration.wav, subtitles.srt
**Processamento**:
```bash
# Construir comando FFmpeg:
ffmpeg -framerate 30 \
       -i images/scene_%03d.png \  # Sequence of images
       -i audio/narration.wav \      # Audio file
       -vf subtitles=subtitles.srt \ # Subtitle filter
       -c:v libx264 -preset slow \   # Video codec
       -crf 18 -c:a aac -b:a 128k \  # Audio codec
       -shortest \                    # Use minimum duration
       video_final.mp4
```

## âš¡ OtimizaÃ§Ãµes

### ParalelizaÃ§Ã£o Futura
Potencial para paralelizar:
- **Step 5** (Imagens): Gerar mÃºltiplas imagens simultaneamente
- Criar um job queue com reutilizaÃ§Ã£o de conexÃ£o Stable Diffusion

### Caching
Implementar em versÃ£o futura:
- Cache de prompts gerados
- Cache de imagens por prompt hash
- Reuso de modelos Ollama em memÃ³ria

### Performance
**Gargalos atuais**:
1. GeraÃ§Ã£o de imagens (Stable Diffusion) â†’ ~50% do tempo total
2. ComposiÃ§Ã£o com FFmpeg â†’ ~20% do tempo

**Melhorias possÃ­veis**:
- GPU mais potente (NVIDIA RTX 4090)
- Usar modelos menores do Stable Diffusion
- Paralelizar geraÃ§Ã£o de imagens

## ğŸ›¡ï¸ ValidaÃ§Ãµes

**Step 1 (Planejamento)**:
```python
required_fields = ["tema", "publico", "ton", "pontos_chave"]
for field in required_fields:
    if field not in plan:
        raise ValueError(f"Campo obrigatÃ³rio faltando: {field}")
```

**Step 2 (Roteiro)**:
```python
num_scenes = script.count("## CENA")
if num_scenes < 3 or num_scenes > 8:
    raise ValueError(f"NÃºmero de cenas invÃ¡lido: {num_scenes}")
```

**Step 5 (Imagens)**:
```python
if not sd.check_connection():
    raise RuntimeError("Stable Diffusion nÃ£o acessÃ­vel")
```

**Step 7 (ComposiÃ§Ã£o)**:
```python
if not Path(output_file).exists():
    raise FileNotFoundError("VÃ­deo nÃ£o foi gerado")
```

## ğŸ“ˆ MÃ©tricas

**SaÃ­das por step**:
- Step 1: 1 arquivo JSON (~2KB)
- Step 2: 1 arquivo Markdown (~1-2KB)
- Step 3: 1 arquivo WAV (~500KB-2MB) + 1 JSON (~1KB)
- Step 4: 1 arquivo JSON (~5-10KB)
- Step 5: N PNGs (~500KB-2MB cada)
- Step 6: 2 arquivos (SRT + VTT, ~5KB cada)
- Step 7: 1 arquivo MP4 (~50-200MB)

**Tempo total esperado**:
- PadrÃ£o: 15-25 minutos
- RÃ¡pido: 10-15 minutos
- Qualidade: 30-45 minutos

## ğŸ” SeguranÃ§a

**ValidaÃ§Ãµes contra injection**:
- Prompts escapados antes de passar para Ollama
- Paths sanitizados antes de usar em FFmpeg
- Sem execuÃ§Ã£o de shell arbitrÃ¡rio (uso de subprocess.run com args separados)

**PermissÃµes de arquivo**:
- CriaÃ§Ã£o automÃ¡tica de diretÃ³rios necessÃ¡rios
- Escritura em output/ apenas
- Leitura de config/ apenas
